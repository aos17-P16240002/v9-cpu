## mTCP：A Highly Scalable User-level TCP Stack for Multicore Systems

##### P15240002 白帅

随着互联网尤其是移动互联网的发展，短报文的TCP连接的比重越来越高。在典型的蜂窝数据流量网络中，90%的TCP流量都小于32KB，超过一半的TCP连接都小于4KB。
高效地处理短连接，对于提升用户体验非常重要，所以也成为了一个研究热点。

这篇文章中，作者提出了一个高效的可扩展在多核机器上用户态TCP协议栈mTCP，使处理短连接的效率有了非常明显的提升。
mTCP易于使用，其提供了和内核非常接近的网络套接字接口，应用只需要很少的改动就可以切换到mTCP，另外mTCP也易于部署，mTCP不涉及对内核的修改，所以部署相对方便。

内核中TCP协议栈的4个主要缺陷：
* 连接的局部性差，多个连接争用端口，产生等待；另外，实际处理TCP报文的CPU核不一定是执行相应应用层程序的CPU核。
* 网络套接字在进程内共享，多线程情况下产生竞争等待
* 单个包的处理处理效率低
* 系统调用的开销过高，尤其是在短连接情况下，会频繁的在内核态和用户态之间进行切换


在Linux内核的TCP协议栈或者MegaPipe中，处理网络报文时大量的时间消耗（80\%）都用在了内核态，而留给用户态的时间很少。所以作者考虑是否能通过用户态的TCP协议栈来提高效率。

mTCP在设计上由这些特点：
* 用户态的协议栈通过用户态的packet I/O库和驱动进行交互，绕过系统调用
* 利用队列对数据包批量处理
* 使用无锁的per-core的数据结构，避免因为锁而带来额外的消耗
* mTCP没有使用系统调用的方式将报文传给进程，而是使用进程不断的轮询查找是否由新的报文到来
* 每个核处理对应进程的报文信息，增强局部性
* 对短连接进行特定优化，提高对小报文的处理优先级，对小报文内存优化

作者通过实验将mTCP和内核协议栈进行了对比，mTCP能够为应用带来320%的网络性能提升。
